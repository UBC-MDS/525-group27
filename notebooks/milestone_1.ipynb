{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RainFall Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Partitioned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/rainfall/partitions/\"\n",
    "\n",
    "\n",
    "# Retrieve the article metadata\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "\n",
    "\n",
    "# Zip to Folder Unzip\n",
    "files_to_dl = [\"data.zip\"]  # feel free to add other files here\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])\n",
    "        \n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "files_to_combine = glob.glob(output_directory + \"*.csv\")\n",
    "files_to_combine.remove(output_directory + \"observed_daily_rainfall_SYD.csv\")\n",
    "df = pd.concat(\n",
    "    (pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'[^\\/&\\\\]+(?=_daily_rainfall_NSW\\.)', file)[0])\n",
    "                for file in files_to_combine)\n",
    "    )\n",
    "\n",
    "# Save the combined data\n",
    "data_path = \"../data/rainfall/\"\n",
    "os.makedirs(data_path + \"combined/\", exist_ok=True)\n",
    "df.to_csv(data_path + \"combined/rainfall_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Combined Data and Simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv(\"../data/rainfall/combined/rainfall_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.448609766"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing different loading methods for EDA timing\n",
    "import sys\n",
    "# size of 'data' in gigabytes\n",
    "sys.getsizeof(data)/(1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time              object\n",
       "lat_min          float64\n",
       "lat_max          float64\n",
       "lon_min          float64\n",
       "lon_max          float64\n",
       "rain (mm/day)    float64\n",
       "model             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3min ± 3.58 s per loop (mean ± std. dev. of 2 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 2 # run statement 3 times for 2 repetitions\n",
    "data.value_counts()\n",
    "\n",
    "data.describe()\n",
    "\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.963029062"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing data types\n",
    "## Changed 'time' from `object` to `datetime.datetime`\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data[['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']] = data[['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']].astype('float32')\n",
    "sys.getsizeof(data)/(1e9) #size of modified data in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 31s ± 315 ms per loop (mean ± std. dev. of 2 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 2 # run statement 3 times for 2 repetitions\n",
    "data.value_counts()\n",
    "\n",
    "data.describe()\n",
    "\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAR MODIFIED DATAFRAME AND RELOAD\n",
    "del data\n",
    "\n",
    "data = pd.read_csv(\"../data/rainfall/combined/rainfall_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.44963879"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using only necessary columns (time, rainfall, model), location columns can be loaded in later if needed\n",
    "\n",
    "data = data[['time', 'rain (mm/day)', 'model']]\n",
    "sys.getsizeof(data)/(1e9) # Very interesting to see here, the location columns account for only ~10% of the total object size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 53s ± 274 ms per loop (mean ± std. dev. of 2 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 2 # run statement 3 times for 2 repetitions\n",
    "data.value_counts()\n",
    "\n",
    "data.describe()\n",
    "\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Team Member | Operating System | RAM | Processor | Is SSD | Time taken for Each Type of Memory Reduction |\n",
    "|:-----------:|:----------------:|:---:|:---------:|:------:|:----------:|\n",
    "| Sam   |    macOS Catalina      |16GB | 2.8 GHz 4 Core i7   |  Yes   | 3mins, 2.5mins, 1.9mins     |\n",
    "| Member 2    |                  |     |           |        |            |\n",
    "| Member 3    |                  |     |           |        |            |\n",
    "| Member 4    |                  |     |           |        |            |\n",
    "\n",
    "I attempted to show some plots but unless I loaded only parts of the dataset, they would not execute. In terms of memory space, changing the data types was the best at reducing the size of the object. This is due to the changing of the numerical data from float64 to float32, which is effectively half the space. The trade-off for doing this is that now our data is less precise than before."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f14e0129284baee33e181c84cc9d0bde2f4d5d150c38a0731cd7f961ea43cb0c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
