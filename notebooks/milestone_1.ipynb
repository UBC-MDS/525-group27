{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RainFall Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Partitioned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/rainfall/partitions/\"\n",
    "\n",
    "\n",
    "# Retrieve the article metadata\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "\n",
    "\n",
    "# Zip to Folder Unzip\n",
    "files_to_dl = [\"data.zip\"]  # feel free to add other files here\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])\n",
    "        \n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "files_to_combine = glob.glob(output_directory + \"*.csv\")\n",
    "files_to_combine.remove(output_directory + \"observed_daily_rainfall_SYD.csv\")\n",
    "df = pd.concat(\n",
    "    (pd.read_csv(file, index_col=0)\n",
    "                .assign(model=re.findall(r'[^\\/&\\\\]+(?=_daily_rainfall_NSW\\.)', file)[0])\n",
    "                for file in files_to_combine)\n",
    "    )\n",
    "\n",
    "# Save the combined data\n",
    "data_path = \"../data/rainfall/\"\n",
    "os.makedirs(data_path + \"combined/\", exist_ok=True)\n",
    "df.to_csv(data_path + \"combined/rainfall_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f14e0129284baee33e181c84cc9d0bde2f4d5d150c38a0731cd7f961ea43cb0c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('563')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
